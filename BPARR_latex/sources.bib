@article{Barab_s_2017,
	title = "Current challenges in autonomous driving",
	author = "I Barab{\'{a}}s and A Todoru{\c{t}} and N Cordo{\c{s}} and A Molea",
	year = "2017",
	month = "10",
	publisher = "{IOP} Publishing",
	volume = "252",
	pages = "012096",
	doi = "10.1088/1757-899x/252/1/012096",
	url = "https://doi.org/10.1088/1757-899x/252/1/012096",
	journal = "{IOP} Conference Series: Materials Science and Engineering"
}

@article{autopilot,
	title = "A Survey on Theories and Applications for Self-Driving Cars Based on Deep Learning Methods",
	author = "Ni, Jianjun and Chen, Yinan and Chen, Yan and Zhu, Jinxiu and Ali, Deena and Cao, Weidong",
	year = "2020",
	month = "04",
	doi = "10.3390/app10082749",
	volume = "10",
	journal = "Applied Sciences",    
}

@article{VIOLET,
	author="McCall, J.C. and Trivedi, M.M.",
	journal="IEEE Transactions on Intelligent Transportation Systems", 
	title="Video-based lane estimation and tracking for driver assistance: survey, system, and evaluation", 
	year="2006",
	volume="7",
	number="1",
	pages="20-37",
	keywords="",
	doi="10.1109/TITS.2006.869595",
	ISSN="1558-0016",
	month="3"
}

@article{eliou_kaliabetsos_2013, 
	title={A new, simple and accurate transition curve type, for use in road and railway alignment design},
	volume={6},
	DOI={10.1007/s12544-013-0119-8},
	number={2},
	journal={European Transport Research Review},
	author={Eliou, Nikolaos and Kaliabetsos, Georgios}, 
	year={2013}, 
	pages={171–179}
}

@article{public_opinion_on_AV,
	author = {Chen, Kaiping and Tomblin, David},
	title = "{Using Data from Reddit, Public Deliberation, and Surveys to Measure Public Opinion about Autonomous Vehicles}",
	journal = {Public Opinion Quarterly},
	volume = {85},
	number = {S1},
	pages = {289-322},
	year = {2021},
	month = {09},
	abstract = "{When and how can researchers synthesize survey data with analyses of social media content to study public opinion, and when and how can social media data complement surveys to better inform researchers and policymakers? This paper explores how public opinions might differ between survey and social media platforms in terms of content and audience, focusing on the test case of opinions about autonomous vehicles. The paper first extends previous overviews comparing surveys and social media as measurement tools to include a broader range of survey types, including surveys that result from public deliberation, considering the dialogic characteristics of different social media, and the range of issue publics and marginalized voices that different surveys and social media forums can attract. It then compares findings and implications from analyses of public opinion about autonomous vehicles from traditional surveys, results of public deliberation, and analyses of Reddit posts, applying a newly developed computational text analysis tool. Findings demonstrate that social media analyses can both help researchers learn more about issues that are uncovered by surveys and also uncover opinions from subpopulations with specialized knowledge and unique orientations toward a subject. In light of these findings, we point to future directions on how researchers and policymakers can synthesize survey and social media data, and the corresponding data integration techniques, to study public opinion.}",
	issn = {0033-362X},
	doi = {10.1093/poq/nfab021},
	url = {https://doi.org/10.1093/poq/nfab021},
	eprint = {https://academic.oup.com/poq/article-pdf/85/S1/289/40514316/nfab021\_supplementary\_data.pdf},
}

@article{AV_vs_CV_crashes,
	title = {Crash comparison of autonomous and conventional vehicles using pre-crash scenario typology},
	journal = {Accident Analysis \& Prevention},
	volume = {159},
	pages = {106281},
	year = {2021},
	issn = {0001-4575},
	doi = {https://doi.org/10.1016/j.aap.2021.106281},
	url = {https://www.sciencedirect.com/science/article/pii/S0001457521003122},
	author = {Qian Liu and Xuesong Wang and Xiangbin Wu and Yi Glaser and Linjia He},
	keywords = {Autonomous vehicle, Conventional vehicle, Pre-crash scenario typology, In-depth crash investigation, Perception-reaction time, Automated driving system},
	abstract = {Data-based research approaches to generate crash scenarios have mainly relied on conventional vehicle crashes and naturalistic driving data, and have not considered differences between the autonomous vehicle (AV) and conventional vehicle crashes. As the AV’s presence on roadways continues to grow, its crash scenarios take on new importance for traffic safety. This study therefore obtained crash patterns using the United States Department of Transportation pre-crash scenario typology, and used statistical analysis to determine the differences between AV and conventional vehicle pre-crash scenarios. Analysis of 122 AV crashes and 2084 conventional vehicle crashes revealed 15 types of scenario for AVs and 26 for conventional vehicles. The two groups showed differences in type of scenario, and differed in the proportion of crashes when the scenario was the same. The most frequent AV pre-crash scenarios were rear-end collisions (52.46\%) and lane change collisions (18.85\%), with the proportion of AVs rear-ended by conventional vehicles occurring with a frequency 1.6 times that of conventional vehicles. An in-depth crash investigation was conducted of the characteristics and causes of four AV pre-crash scenarios, summarized from the perspectives of perception and path planning. The perception-reaction time (PRT) difference between AVs and human drivers, AV’s inaccurate identification of the intention of other vehicles to change lanes, and AV’s insufficient path planning combining time and space dimensions were found to be important causes for the AV crashes. By increasing understanding of the complex characteristics of AV pre-crash scenarios, this analysis will encourage cooperation with vehicle manufacturers and AV technology companies for further study of crash causation toward the goals of improved test scenario construction and optimization of the AV’s automated driving system (ADS).}
}

@article{AV_crashes_involved_vulnerable,
	title = {Mining patterns of autonomous vehicle crashes involving vulnerable road users to understand the associated factors},
	journal = {Accident Analysis \& Prevention},
	volume = {165},
	pages = {106473},
	year = {2022},
	issn = {0001-4575},
	doi = {https://doi.org/10.1016/j.aap.2021.106473},
	url = {https://www.sciencedirect.com/science/article/pii/S0001457521005042},
	author = {Boniphace Kutela and Subasish Das and Bahar Dadashova},
	keywords = {Autonomous vehicles crashes, Vulnerable road users, Crash narratives, Text network, Text classifiers},
	abstract = {Autonomous or automated vehicles (AVs) have the potential to improve traffic safety by eliminating majority of human errors. As the interest in AV deployment increases, there is an increasing need to assess and understand the expected implications of AVs on traffic safety. Until recently, most of the literature has been based on either survey questionnaires, simulation analysis, virtual reality, or simulation to assess the safety benefits of AVs. Although few studies have used AV crash data, vulnerable road users (VRUs) have not been a topic of interest. Therefore, this study uses crash narratives from four-year (2017–2020) of AV crash data collected from California to explore the direct and indirect involvement of VRUs. The study applied text network and compared the text classification performance of four classifiers - Support Vector Machine (SVM), Naïve Bayes (NB), Random Forest (RF), and Neural Network (NN) and associated performance metrics to attain the objective. It was found that out of 252 crashes, VRUs were, directly and indirectly, involved in 23 and 12 crashes, respectively. Among VRUs, bicyclists and scooterists are more likely to be involved in the AV crashes directly, and bicyclists are likely to be at fault, while pedestrians appear more in the indirectly involvements. Further, crashes that involve VRUs indirectly are likely to occur when the AVs are in autonomous mode and are slightly involved minor damages on the rear bumper than the ones that directly involve VRUs. Additionally, feature importance from the best performing classifiers (RF and NN) revealed that crosswalks, intersections, traffic signals, movements of AVs (turning, slowing down, stopping) are the key predictors of the VRUs-AV related crashes. These findings can be helpful to AV operators and city planners.}
}

@manual{SAE_autonomy_levels,
	author={On-Road Automated Driving (ORAD) Committee},
	title={Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles},
	month={4},
	year={2021},
	doi={https://doi.org/10.4271/J3016_202104},
	url={https://doi.org/10.4271/J3016_202104},
	abstract={This document describes [motor] vehicle driving automation systems that perform part or all of the dynamic driving task (DDT) on a sustained basis. It provides a taxonomy with detailed definitions for six levels of driving automation, ranging from no driving automation (Level 0) to full driving automation (Level 5), in the context of [motor] vehicles (hereafter also referred to as “vehicle” or “vehicles”) and their operation on roadways: Level 0: No Driving Automation Level 1: Driver Assistance Level 2: Partial Driving Automation Level 3: Conditional Driving Automation Level 4: High Driving Automation Level 5: Full Driving Automation These level definitions, along with additional supporting terms and definitions provided herein, can be used to describe the full range of driving automation features equipped on [motor] vehicles in a functionally consistent and coherent manner. “On-road” refers to publicly accessible roadways (including parking areas and private campuses that permit public access) that collectively serve all road users, including cyclists, pedestrians, and users of vehicles with and without driving automation features. The levels apply to the driving automation feature(s) that are engaged in any given instance of on-road operation of an equipped vehicle. As such, although a given vehicle may be equipped with a driving automation system that is capable of delivering multiple driving automation features that perform at different levels, the level of driving automation exhibited in any given instance is determined by the feature(s) that are engaged. This document also refers to three primary actors in driving: the (human) user, the driving automation system, and other vehicle systems and components. These other vehicle systems and components (or the vehicle in general terms) do not include the driving automation system in this model, even though as a practical matter a driving automation system may actually share hardware and software components with other vehicle systems, such as a processing module(s) or operating code. The levels of driving automation are defined by reference to the specific role played by each of the three primary actors in performance of the DDT and/or DDT fallback. “Role” in this context refers to the expected role of a given primary actor, based on the design of the driving automation system in question and not necessarily to the actual performance of a given primary actor. For example, a driver who fails to monitor the roadway during engagement of a Level 1 adaptive cruise control (ACC) system still has the role of driver, even while s/he is neglecting it. Active safety systems, such as electronic stability control (ESC) and automatic emergency braking (AEB), and certain types of driver assistance systems, such as lane keeping assistance (LKA), are excluded from the scope of this driving automation taxonomy because they do not perform part or all of the DDT on a sustained basis, but rather provide momentary intervention during potentially hazardous situations. Due to the momentary nature of the actions of active safety systems, their intervention does not change or eliminate the role of the driver in performing part or all of the DDT, and thus are not considered to be driving automation, even though they perform automated functions. In addition, systems that inform, alert, or warn the driver about hazards in the driving environment are also outside the scope of this driving automation taxonomy, as they neither automate part or all of the DDT, nor change the driver’s role in performance of the DDT (see 8.13). It should be noted, however, that crash avoidance features, including intervention-type active safety systems, may be included in vehicles equipped with driving automation systems at any level. For automated driving system (ADS) features (i.e., Levels 3 to 5) that perform the complete DDT, crash mitigation and avoidance capability is part of ADS functionality (see also 8.13).}
}

@article{2D_convolution,
	title={Applications of convolution in image processing with MATLAB},
	author={Kim, Sung and Casper, Riley},
	journal={University of Washington},
	pages={1--20},
	year={2013}
}

@article{steerable_filters,
	title={The design and use of steerable filters},
	author={Freeman, William T and Adelson, Edward H and others},
	journal={IEEE Transactions on Pattern analysis and machine intelligence},
	volume={13},
	number={9},
	pages={891--906},
	year={1991}
}

@article{sobel_filter_history,
	author = {Sobel, Irwin},
	year = {2014},
	month = {02},
	pages = {},
	title = {An Isotropic 3x3 Image Gradient Operator},
	journal = {Presentation at Stanford A.I. Project 1968}
}

@article{polyfit,
	title = {Polyfit — A package for polynomial fitting},
	journal = {Computer Physics Communications},
	volume = {52},
	number = {3},
	pages = {427-442},
	year = {1989},
	issn = {0010-4655},
	doi = {https://doi.org/10.1016/0010-4655(89)90117-3},
	url = {https://www.sciencedirect.com/science/article/pii/0010465589901173},
	author = {S.J. Sciutto},
	abstract = {Polynomials P(x) of arbitrary degree are fitted with or without constraints, to a given set of points (Xn, Yn), n = 1,…,N; N > 1, using the least squares method. A weight ωn may be assigned to each point and/or an error — or fluctuation — ΔYn to the respective Yn's. Two kinds of constraints may be specified: 1) Fixed subset of the polynomial's coefficients and 2) P(ξq) = ηq, q = 1,…,Q, Q > 0 for given points (ξq, ηq), q = 1,…,Q. The problem is reduced to a system of linear equations solved by the Gauss reduction technique.}
}

@book{RLbook,
	title={Reinforcement learning: An introduction}, 
	publisher={The MIT Press}, 
	author={Sutton, Richard S. and Barto, Andrew}, 
	year={2020}
}

@article{vanilla_Q-learning,
	title={Learning from delayed rewards},
	author={Watkins, Christopher John Cornish Hellaby},
	year={1989},
	publisher={King's College, Cambridge United Kingdom}
}

@article{Deep_RL_survey,
	author={Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Sallab, Ahmad A. Al and Yogamani, Senthil and Pérez, Patrick},
	journal={IEEE Transactions on Intelligent Transportation Systems}, 
	title={Deep Reinforcement Learning for Autonomous Driving: A Survey}, 
	year={2022},
	volume={23},
	number={6},
	pages={4909-4926},
	abstract={With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.},
	keywords={},
	doi={10.1109/TITS.2021.3054625},
	ISSN={1558-0016},
	month={6},
}

@article{A2C,
	title = {Natural actor–critic algorithms},
	journal = {Automatica},
	volume = {45},
	number = {11},
	pages = {2471-2482},
	year = {2009},
	issn = {0005-1098},
	doi = {https://doi.org/10.1016/j.automatica.2009.07.008},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109809003549},
	author = {Shalabh Bhatnagar and Richard S. Sutton and Mohammad Ghavamzadeh and Mark Lee},
	keywords = {Actor–critic reinforcement learning algorithms, Policy-gradient methods, Approximate dynamic programming, Function approximation, Two-timescale stochastic approximation, Temporal difference learning, Natural gradient},
	abstract = {We present four new reinforcement learning algorithms based on actor–critic, natural-gradient and function-approximation ideas, and we provide their convergence proofs. Actor–critic reinforcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their compatibility with function-approximation methods, which are needed to handle large or infinite state spaces. The use of temporal difference learning in this way is of special interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases. Our results extend prior two-timescale convergence results for actor–critic methods by Konda and Tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients. Our results extend prior empirical studies of natural actor–critic methods by Peters, Vijayakumar and Schaal by providing the first convergence proofs and the first fully incremental algorithms.}
}

@article{LeNet-5,
	author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	journal={Proceedings of the IEEE}, 
	title={Gradient-based learning applied to document recognition}, 
	year={1998},
	volume={86},
	number={11},
	pages={2278-2324},
abstract={Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	doi={10.1109/5.726791},
	ISSN={1558-2256},
	month={11},
}

@inproceedings{ImageNet,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	volume = {25},
	year = {2012}
}

@misc{VGG-16,
	title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
	author={Karen Simonyan and Andrew Zisserman},
	year={2015},
	eprint={1409.1556},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	doi={https://doi.org/10.48550/arXiv.1409.1556}
}

@misc{Inception-v3,
	title={Rethinking the Inception Architecture for Computer Vision}, 
	author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
	year={2015},
	eprint={1512.00567},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	doi={https://doi.org/10.48550/arXiv.1512.00567}
}

@misc{ResNet-50,
	title={Deep Residual Learning for Image Recognition}, 
	author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year={2015},
	eprint={1512.03385},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	doi={https://doi.org/10.48550/arXiv.1512.03385}
}

@misc{Network_in_network,
	title={Network In Network}, 
	author={Min Lin and Qiang Chen and Shuicheng Yan},
	year={2014},
	eprint={1312.4400},
	archivePrefix={arXiv},
	primaryClass={cs.NE},
	doi={https://doi.org/10.48550/arXiv.1312.4400}
}

@INPROCEEDINGS{YOLO,
author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={You Only Look Once: Unified, Real-Time Object Detection}, 
year={2016},
pages={779-788},
abstract={We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
keywords={},
doi={10.1109/CVPR.2016.91},
ISSN={1063-6919},
month={6},
}

@INPROCEEDINGS{R-CNN,
	author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
	title={Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}, 
	year={2014},
	pages={580-587},
	abstract={Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
	doi={10.1109/CVPR.2014.81},
	ISSN={1063-6919},
	month={6},
}

@misc{U-Net,
	title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
	author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
	year={2015},
	eprint={1505.04597},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	doi={https://doi.org/10.48550/arXiv.1505.04597}
}

@article{semantic_segmentation_survey,
	title = {A Brief Survey on Semantic Segmentation with Deep Learning},
	journal = {Neurocomputing},
	volume = {406},
	pages = {302-321},
	year = {2020},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2019.11.118},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220305476},
	author = {Shijie Hao and Yuan Zhou and Yanrong Guo},
	keywords = {Semantic segmentation, Deep learning},
	abstract = {Semantic segmentation is a challenging task in computer vision. In recent years, the performance of semantic segmentation has been greatly improved by using deep learning techniques. A large number of novel methods have been proposed. This paper aims to provide a brief review of research efforts on deep-learning-based semantic segmentation methods. We categorize the related research according to its supervision level, i.e., fully-supervised methods, weakly-supervised methods and semi-supervised methods. We also discuss the common challenges of the current research, and present several valuable growing research points in this field. This survey is expected to familiarize readers with the progress and challenges of semantic segmentation research in the deep learning era.}
}