@article{Barab_s_2017,
	title = "Current challenges in autonomous driving",
	author = "I Barab{\'{a}}s and A Todoru{\c{t}} and N Cordo{\c{s}} and A Molea",
	year = "2017",
	month = "10",
	publisher = "{IOP} Publishing",
	volume = "252",
	pages = "012096",
	doi = "10.1088/1757-899x/252/1/012096",
	url = "https://doi.org/10.1088/1757-899x/252/1/012096",
	journal = "{IOP} Conference Series: Materials Science and Engineering"
}

@article{autopilot,
	title = "A Survey on Theories and Applications for Self-Driving Cars Based on Deep Learning Methods",
	author = "Ni, Jianjun and Chen, Yinan and Chen, Yan and Zhu, Jinxiu and Ali, Deena and Cao, Weidong",
	year = "2020",
	month = "04",
	doi = "10.3390/app10082749",
	volume = "10",
	journal = "Applied Sciences",    
}

@article{VIOLET,
	author="McCall, J.C. and Trivedi, M.M.",
	journal="IEEE Transactions on Intelligent Transportation Systems", 
	title="Video-based lane estimation and tracking for driver assistance: survey, system, and evaluation", 
	year="2006",
	volume="7",
	number="1",
	pages="20-37",
	keywords="",
	doi="10.1109/TITS.2006.869595",
	ISSN="1558-0016",
	month="3"
}

@article{eliou_kaliabetsos_2013, 
	title={A new, simple and accurate transition curve type, for use in road and railway alignment design},
	volume={6},
	DOI={10.1007/s12544-013-0119-8},
	number={2},
	journal={European Transport Research Review},
	author={Eliou, Nikolaos and Kaliabetsos, Georgios}, 
	year={2013}, 
	pages={171–179}
}

@article{public_opinion_on_AV,
	author = {Chen, Kaiping and Tomblin, David},
	title = "{Using Data from Reddit, Public Deliberation, and Surveys to Measure Public Opinion about Autonomous Vehicles}",
	journal = {Public Opinion Quarterly},
	volume = {85},
	number = {S1},
	pages = {289-322},
	year = {2021},
	month = {09},
	abstract = "{When and how can researchers synthesize survey data with analyses of social media content to study public opinion, and when and how can social media data complement surveys to better inform researchers and policymakers? This paper explores how public opinions might differ between survey and social media platforms in terms of content and audience, focusing on the test case of opinions about autonomous vehicles. The paper first extends previous overviews comparing surveys and social media as measurement tools to include a broader range of survey types, including surveys that result from public deliberation, considering the dialogic characteristics of different social media, and the range of issue publics and marginalized voices that different surveys and social media forums can attract. It then compares findings and implications from analyses of public opinion about autonomous vehicles from traditional surveys, results of public deliberation, and analyses of Reddit posts, applying a newly developed computational text analysis tool. Findings demonstrate that social media analyses can both help researchers learn more about issues that are uncovered by surveys and also uncover opinions from subpopulations with specialized knowledge and unique orientations toward a subject. In light of these findings, we point to future directions on how researchers and policymakers can synthesize survey and social media data, and the corresponding data integration techniques, to study public opinion.}",
	issn = {0033-362X},
	doi = {10.1093/poq/nfab021},
	url = {https://doi.org/10.1093/poq/nfab021},
	eprint = {https://academic.oup.com/poq/article-pdf/85/S1/289/40514316/nfab021\_supplementary\_data.pdf},
}

@article{AV_vs_CV_crashes,
	title = {Crash comparison of autonomous and conventional vehicles using pre-crash scenario typology},
	journal = {Accident Analysis \& Prevention},
	volume = {159},
	pages = {106281},
	year = {2021},
	issn = {0001-4575},
	doi = {https://doi.org/10.1016/j.aap.2021.106281},
	url = {https://www.sciencedirect.com/science/article/pii/S0001457521003122},
	author = {Qian Liu and Xuesong Wang and Xiangbin Wu and Yi Glaser and Linjia He},
	keywords = {Autonomous vehicle, Conventional vehicle, Pre-crash scenario typology, In-depth crash investigation, Perception-reaction time, Automated driving system},
	abstract = {Data-based research approaches to generate crash scenarios have mainly relied on conventional vehicle crashes and naturalistic driving data, and have not considered differences between the autonomous vehicle (AV) and conventional vehicle crashes. As the AV’s presence on roadways continues to grow, its crash scenarios take on new importance for traffic safety. This study therefore obtained crash patterns using the United States Department of Transportation pre-crash scenario typology, and used statistical analysis to determine the differences between AV and conventional vehicle pre-crash scenarios. Analysis of 122 AV crashes and 2084 conventional vehicle crashes revealed 15 types of scenario for AVs and 26 for conventional vehicles. The two groups showed differences in type of scenario, and differed in the proportion of crashes when the scenario was the same. The most frequent AV pre-crash scenarios were rear-end collisions (52.46\%) and lane change collisions (18.85\%), with the proportion of AVs rear-ended by conventional vehicles occurring with a frequency 1.6 times that of conventional vehicles. An in-depth crash investigation was conducted of the characteristics and causes of four AV pre-crash scenarios, summarized from the perspectives of perception and path planning. The perception-reaction time (PRT) difference between AVs and human drivers, AV’s inaccurate identification of the intention of other vehicles to change lanes, and AV’s insufficient path planning combining time and space dimensions were found to be important causes for the AV crashes. By increasing understanding of the complex characteristics of AV pre-crash scenarios, this analysis will encourage cooperation with vehicle manufacturers and AV technology companies for further study of crash causation toward the goals of improved test scenario construction and optimization of the AV’s automated driving system (ADS).}
}

@article{AV_crashes_involved_vulnerable,
	title = {Mining patterns of autonomous vehicle crashes involving vulnerable road users to understand the associated factors},
	journal = {Accident Analysis \& Prevention},
	volume = {165},
	pages = {106473},
	year = {2022},
	issn = {0001-4575},
	doi = {https://doi.org/10.1016/j.aap.2021.106473},
	url = {https://www.sciencedirect.com/science/article/pii/S0001457521005042},
	author = {Boniphace Kutela and Subasish Das and Bahar Dadashova},
	keywords = {Autonomous vehicles crashes, Vulnerable road users, Crash narratives, Text network, Text classifiers},
	abstract = {Autonomous or automated vehicles (AVs) have the potential to improve traffic safety by eliminating majority of human errors. As the interest in AV deployment increases, there is an increasing need to assess and understand the expected implications of AVs on traffic safety. Until recently, most of the literature has been based on either survey questionnaires, simulation analysis, virtual reality, or simulation to assess the safety benefits of AVs. Although few studies have used AV crash data, vulnerable road users (VRUs) have not been a topic of interest. Therefore, this study uses crash narratives from four-year (2017–2020) of AV crash data collected from California to explore the direct and indirect involvement of VRUs. The study applied text network and compared the text classification performance of four classifiers - Support Vector Machine (SVM), Naïve Bayes (NB), Random Forest (RF), and Neural Network (NN) and associated performance metrics to attain the objective. It was found that out of 252 crashes, VRUs were, directly and indirectly, involved in 23 and 12 crashes, respectively. Among VRUs, bicyclists and scooterists are more likely to be involved in the AV crashes directly, and bicyclists are likely to be at fault, while pedestrians appear more in the indirectly involvements. Further, crashes that involve VRUs indirectly are likely to occur when the AVs are in autonomous mode and are slightly involved minor damages on the rear bumper than the ones that directly involve VRUs. Additionally, feature importance from the best performing classifiers (RF and NN) revealed that crosswalks, intersections, traffic signals, movements of AVs (turning, slowing down, stopping) are the key predictors of the VRUs-AV related crashes. These findings can be helpful to AV operators and city planners.}
}

@manual{SAE_autonomy_levels,
	author={On-Road Automated Driving (ORAD) Committee},
	title={Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles},
	month={4},
	year={2021},
	doi={https://doi.org/10.4271/J3016_202104},
	url={https://doi.org/10.4271/J3016_202104},
	abstract={This document describes [motor] vehicle driving automation systems that perform part or all of the dynamic driving task (DDT) on a sustained basis. It provides a taxonomy with detailed definitions for six levels of driving automation, ranging from no driving automation (Level 0) to full driving automation (Level 5), in the context of [motor] vehicles (hereafter also referred to as “vehicle” or “vehicles”) and their operation on roadways: Level 0: No Driving Automation Level 1: Driver Assistance Level 2: Partial Driving Automation Level 3: Conditional Driving Automation Level 4: High Driving Automation Level 5: Full Driving Automation These level definitions, along with additional supporting terms and definitions provided herein, can be used to describe the full range of driving automation features equipped on [motor] vehicles in a functionally consistent and coherent manner. “On-road” refers to publicly accessible roadways (including parking areas and private campuses that permit public access) that collectively serve all road users, including cyclists, pedestrians, and users of vehicles with and without driving automation features. The levels apply to the driving automation feature(s) that are engaged in any given instance of on-road operation of an equipped vehicle. As such, although a given vehicle may be equipped with a driving automation system that is capable of delivering multiple driving automation features that perform at different levels, the level of driving automation exhibited in any given instance is determined by the feature(s) that are engaged. This document also refers to three primary actors in driving: the (human) user, the driving automation system, and other vehicle systems and components. These other vehicle systems and components (or the vehicle in general terms) do not include the driving automation system in this model, even though as a practical matter a driving automation system may actually share hardware and software components with other vehicle systems, such as a processing module(s) or operating code. The levels of driving automation are defined by reference to the specific role played by each of the three primary actors in performance of the DDT and/or DDT fallback. “Role” in this context refers to the expected role of a given primary actor, based on the design of the driving automation system in question and not necessarily to the actual performance of a given primary actor. For example, a driver who fails to monitor the roadway during engagement of a Level 1 adaptive cruise control (ACC) system still has the role of driver, even while s/he is neglecting it. Active safety systems, such as electronic stability control (ESC) and automatic emergency braking (AEB), and certain types of driver assistance systems, such as lane keeping assistance (LKA), are excluded from the scope of this driving automation taxonomy because they do not perform part or all of the DDT on a sustained basis, but rather provide momentary intervention during potentially hazardous situations. Due to the momentary nature of the actions of active safety systems, their intervention does not change or eliminate the role of the driver in performing part or all of the DDT, and thus are not considered to be driving automation, even though they perform automated functions. In addition, systems that inform, alert, or warn the driver about hazards in the driving environment are also outside the scope of this driving automation taxonomy, as they neither automate part or all of the DDT, nor change the driver’s role in performance of the DDT (see 8.13). It should be noted, however, that crash avoidance features, including intervention-type active safety systems, may be included in vehicles equipped with driving automation systems at any level. For automated driving system (ADS) features (i.e., Levels 3 to 5) that perform the complete DDT, crash mitigation and avoidance capability is part of ADS functionality (see also 8.13).}
}

@article{2D_convolution,
	title={Applications of convolution in image processing with MATLAB},
	author={Kim, Sung and Casper, Riley},
	journal={University of Washington},
	pages={1--20},
	year={2013}
}

@article{steerable_filters,
	title={The design and use of steerable filters},
	author={Freeman, William T and Adelson, Edward H and others},
	journal={IEEE Transactions on Pattern analysis and machine intelligence},
	volume={13},
	number={9},
	pages={891--906},
	year={1991}
}

@article{sobel_filter_history,
	author = {Sobel, Irwin},
	year = {2014},
	month = {02},
	pages = {},
	title = {An Isotropic 3x3 Image Gradient Operator},
	journal = {Presentation at Stanford A.I. Project 1968}
}

@article{polyfit,
	title = {Polyfit — A package for polynomial fitting},
	journal = {Computer Physics Communications},
	volume = {52},
	number = {3},
	pages = {427-442},
	year = {1989},
	issn = {0010-4655},
	doi = {https://doi.org/10.1016/0010-4655(89)90117-3},
	url = {https://www.sciencedirect.com/science/article/pii/0010465589901173},
	author = {S.J. Sciutto},
	abstract = {Polynomials P(x) of arbitrary degree are fitted with or without constraints, to a given set of points (Xn, Yn), n = 1,…,N; N > 1, using the least squares method. A weight ωn may be assigned to each point and/or an error — or fluctuation — ΔYn to the respective Yn's. Two kinds of constraints may be specified: 1) Fixed subset of the polynomial's coefficients and 2) P(ξq) = ηq, q = 1,…,Q, Q > 0 for given points (ξq, ηq), q = 1,…,Q. The problem is reduced to a system of linear equations solved by the Gauss reduction technique.}
}

@book{RLbook,
	title={Reinforcement learning: An introduction}, 
	publisher={The MIT Press}, 
	author={Sutton, Richard S. and Barto, Andrew}, 
	year={2020}
}

@article{vanilla_Q-learning,
	title={Learning from delayed rewards},
	author={Watkins, Christopher John Cornish Hellaby},
	year={1989},
	publisher={King's College, Cambridge United Kingdom}
}

@article{Deep_RL_survey,
	author={Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Sallab, Ahmad A. Al and Yogamani, Senthil and Pérez, Patrick},
	journal={IEEE Transactions on Intelligent Transportation Systems}, 
	title={Deep Reinforcement Learning for Autonomous Driving: A Survey}, 
	year={2022},
	volume={23},
	number={6},
	pages={4909-4926},
	abstract={With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.},
	keywords={},
	doi={10.1109/TITS.2021.3054625},
	ISSN={1558-0016},
	month={6},
}

@article{A2C,
	title = {Natural actor–critic algorithms},
	journal = {Automatica},
	volume = {45},
	number = {11},
	pages = {2471-2482},
	year = {2009},
	issn = {0005-1098},
	doi = {https://doi.org/10.1016/j.automatica.2009.07.008},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109809003549},
	author = {Shalabh Bhatnagar and Richard S. Sutton and Mohammad Ghavamzadeh and Mark Lee},
	keywords = {Actor–critic reinforcement learning algorithms, Policy-gradient methods, Approximate dynamic programming, Function approximation, Two-timescale stochastic approximation, Temporal difference learning, Natural gradient},
	abstract = {We present four new reinforcement learning algorithms based on actor–critic, natural-gradient and function-approximation ideas, and we provide their convergence proofs. Actor–critic reinforcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their compatibility with function-approximation methods, which are needed to handle large or infinite state spaces. The use of temporal difference learning in this way is of special interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases. Our results extend prior two-timescale convergence results for actor–critic methods by Konda and Tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients. Our results extend prior empirical studies of natural actor–critic methods by Peters, Vijayakumar and Schaal by providing the first convergence proofs and the first fully incremental algorithms.}
}

@article{LeNet-5,
	author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	journal={Proceedings of the IEEE}, 
	title={Gradient-based learning applied to document recognition}, 
	year={1998},
	volume={86},
	number={11},
	pages={2278-2324},
abstract={Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	doi={10.1109/5.726791},
	ISSN={1558-2256},
	month={11},
}

@inproceedings{ImageNet,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	volume = {25},
	year = {2012}
}

@misc{VGG-16,
	title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
	author={Karen Simonyan and Andrew Zisserman},
	year={2015},
	eprint={1409.1556},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	doi={https://doi.org/10.48550/arXiv.1409.1556}
}

@misc{Inception-v3,
	title={Rethinking the Inception Architecture for Computer Vision}, 
	author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
	year={2015},
	eprint={1512.00567},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	doi={https://doi.org/10.48550/arXiv.1512.00567}
}

@misc{ResNet-50,
	title={Deep Residual Learning for Image Recognition}, 
	author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year={2015},
	eprint={1512.03385},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	doi={https://doi.org/10.48550/arXiv.1512.03385}
}

@misc{Network_in_network,
	title={Network In Network}, 
	author={Min Lin and Qiang Chen and Shuicheng Yan},
	year={2014},
	eprint={1312.4400},
	archivePrefix={arXiv},
	primaryClass={cs.NE},
	doi={https://doi.org/10.48550/arXiv.1312.4400}
}

@INPROCEEDINGS{YOLO,
author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={You Only Look Once: Unified, Real-Time Object Detection}, 
year={2016},
pages={779-788},
abstract={We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
keywords={},
doi={10.1109/CVPR.2016.91},
ISSN={1063-6919},
month={6},
}

@INPROCEEDINGS{R-CNN,
	author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
	title={Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}, 
	year={2014},
	pages={580-587},
	abstract={Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
	doi={10.1109/CVPR.2014.81},
	ISSN={1063-6919},
	month={6},
}

@misc{U-Net,
	title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
	author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
	year={2015},
	eprint={1505.04597},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	doi={https://doi.org/10.48550/arXiv.1505.04597}
}

@article{semantic_segmentation_survey,
	title = {A Brief Survey on Semantic Segmentation with Deep Learning},
	journal = {Neurocomputing},
	volume = {406},
	pages = {302-321},
	year = {2020},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2019.11.118},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220305476},
	author = {Shijie Hao and Yuan Zhou and Yanrong Guo},
	keywords = {Semantic segmentation, Deep learning},
	abstract = {Semantic segmentation is a challenging task in computer vision. In recent years, the performance of semantic segmentation has been greatly improved by using deep learning techniques. A large number of novel methods have been proposed. This paper aims to provide a brief review of research efforts on deep-learning-based semantic segmentation methods. We categorize the related research according to its supervision level, i.e., fully-supervised methods, weakly-supervised methods and semi-supervised methods. We also discuss the common challenges of the current research, and present several valuable growing research points in this field. This survey is expected to familiarize readers with the progress and challenges of semantic segmentation research in the deep learning era.}
}

@InProceedings{simulator_Carla,
  title = 	 {{CARLA}: {An} Open Urban Driving Simulator},
  author = 	 {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
  booktitle = 	 {Proceedings of the 1st Annual Conference on Robot Learning},
  pages = 	 {1--16},
  year = 	 {2017},
  editor = 	 {Levine, Sergey and Vanhoucke, Vincent and Goldberg, Ken},
  volume = 	 {78},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf},
  url = 	 {https://proceedings.mlr.press/v78/dosovitskiy17a.html},
  abstract = 	 {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform’s utility for autonomous driving research.}
}

@InProceedings{Duckietown,
	author={Tani, Jacopo and Paull, Liam
and Zuber, Maria T.
and Rus, Daniela
and How, Jonathan
and Leonard, John
and Censi, Andrea},
	editor="Alimisis, Dimitris
	and Moro, Michele
	and Menegatti, Emanuele",
	title="Duckietown: An Innovative Way to Teach Autonomy",
	booktitle="Educational Robotics in the Makers Era",
	year="2017",
	publisher="Springer International Publishing",
	address="Cham",
	pages="104--121",
	abstract="Teaching robotics is challenging because it is a multidisciplinary, rapidly evolving and experimental discipline that integrates cutting-edge hardware and software. This paper describes the course design and first implementation of Duckietown, a vehicle autonomy class that experiments with teaching innovations in addition to leveraging modern educational theory for improving student learning. We provide a robot to every student, thanks to a minimalist platform design, to maximize active learning; and introduce a role-play aspect to increase team spirit, by modeling the entire class as a fictional start-up (Duckietown Engineering Co.). The course formulation leverages backward design by formalizing intended learning outcomes (ILOs) enabling students to appreciate the challenges of: (a) heterogeneous disciplines converging in the design of a minimal self-driving car, (b) integrating subsystems to create complex system behaviors, and (c) allocating constrained computational resources. Students learn how to assemble, program, test and operate a self-driving car (Duckiebot) in a model urban environment (Duckietown), as well as how to implement and document new features in the system. Traditional course assessment tools are complemented by a full scale demonstration to the general public. The ``duckie'' theme was chosen to give a gender-neutral, friendly identity to the robots so as to improve student involvement and outreach possibilities. All of the teaching materials and code is released online in the hope that other institutions will adopt the platform and continue to evolve and improve it, so to keep pace with the fast evolution of the field.",
	isbn="978-3-319-55553-9",
	doi={https://doi.org/10.1007/978-3-319-55553-9_8},
}

@misc{gym_duckietown,
  author = {Chevalier-Boisvert, Maxime and Golemo, Florian and Cao, Yanjun and Mehta, Bhairav and Paull, Liam},
  title = {Duckietown Environments for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/duckietown/gym-duckietown}},
}

@article{AI-DO,
  title={The AI Driving Olympics at NeurIPS 2018},
  author={Julian Zilly and Jacopo Tani and Breandan Considine and Bhairav Mehta and Andrea F. Daniele and Manfred Diaz and Gianmarco Bernasconi and Claudio Ruch and Jan Hakenberg and Florian Golemo and A. Kirsten Bowser and Matthew R. Walter and Ruslan Hristov and Sunil Mallya and Emilio Frazzoli and Andrea Censi and Liam Paull},
  journal={arXiv preprint arXiv:1903.02503},
  year={2019}
}

@inproceedings{airsim_paper,
  author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor},
  title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},
  year = {2017},
  booktitle = {Field and Service Robotics},
  eprint = {arXiv:1705.05065},
  url = {https://arxiv.org/abs/1705.05065}
}

@article{woodscape,
  title={WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving},
  author={Yogamani, Senthil and Hughes, Ciar{\'a}n and Horgan, Jonathan and Sistu, Ganesh and Varley, Padraig and O'Dea, Derek and Uric{\'a}r, Michal and Milz, Stefan and Simon, Martin and Amende, Karl and others},
  journal={arXiv preprint arXiv:1905.01489},
  year={2019}
}

@article{a2d2,
title={{A2D2: Audi Autonomous Driving Dataset}},
author={Jakob Geyer and Yohannes Kassahun and Mentar Mahmudi and Xavier Ricou and Rupesh Durgesh and Andrew S. Chung and Lorenz Hauswald and Viet Hoang Pham and Maximilian M{\"u}hlegg and Sebastian Dorn and Tiffany Fernandez and Martin J{\"a}nicke and Sudesh Mirashi and Chiragkumar Savani and Martin Sturm and Oleksandr Vorobiov and Martin Oelker and Sebastian Garreis and Peter Schuberth},
year={2020},
eprint={2004.06320},
archivePrefix={arXiv},
primaryClass={cs.CV},
url = {https://www.a2d2.audi}
}

@misc{bdd100k,
      title={BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning}, 
      author={Fisher Yu and Haofeng Chen and Xin Wang and Wenqi Xian and Yingying Chen and Fangchen Liu and Vashisht Madhavan and Trevor Darrell},
      year={2020},
      eprint={1805.04687},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
	doi={https://doi.org/10.48550/arXiv.1805.04687}
}

@book{neural_networks_background_01,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@misc{neural_networks_background_02,
	title={Neural Networks and Deep Learning},
	url={Michael A. Nielsen},
	publisher={Determination Press},
	year={2015}
}

% internetové odkazy
@misc{SAE_autonomy_levels_web,
	author={SAE},
	year={2021},
	url={https://www.sae.org/standards/content/j3016_202104}
}

@misc{duckietown_web,
	title={Duckietown project web},
	url={https://www.duckietown.org/},
	urldate={2023-07-26}
}

@misc{duckietown_documentation,
	title={The Duckietown library},
	urldate = {2023-07-26},
	url={https://docs-old.duckietown.org/daffy/}
}

@misc{Kaggle,
	title={Kaggle},
	url={https://www.kaggle.com/},
	urldate={2023-07-26}
}

@misc{Roboflow,
	title={Roboflow},
	url={https://public.roboflow.com/},
	urldate={2023-07-26}
}

@misc{Apollo,
	title={Apollo projekt},
	url={https://www.apollo.auto/},
	urldate={2023-07-26}
}


% datasety
@misc{dataset_duckietown,
	url={https://docs-old.duckietown.org/daffy/AIDO/out/object_detection_dataset.html},
	urldate={2023-07-26}
}

@INPROCEEDINGS{dataset_KITTI_road,
	author={Fritsch, Jannik and Kühnl, Tobias and Geiger, Andreas},
	booktitle={16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)}, 
	title={A new performance measure and evaluation benchmark for road detection algorithms}, 
	year={2013},
	volume={},
	number={},
	pages={1693-1700},
	abstract={Detecting the road area and ego-lane ahead of a vehicle is central to modern driver assistance systems. While lane-detection on well-marked roads is already available in modern vehicles, finding the boundaries of unmarked or weakly marked roads and lanes as they appear in inner-city and rural environments remains an unsolved problem due to the high variability in scene layout and illumination conditions, amongst others. While recent years have witnessed great interest in this subject, to date no commonly agreed upon benchmark exists, rendering a fair comparison amongst methods difficult. In this paper, we introduce a novel open-access dataset and benchmark for road area and ego-lane detection. Our dataset comprises 600 annotated training and test images of high variability from the KITTI autonomous driving project, capturing a broad spectrum of urban road scenes. For evaluation, we propose to use the 2D Bird's Eye View (BEV) space as vehicle control usually happens in this 2D world, requiring detection results to be represented in this very same space. Furthermore, we propose a novel, behavior-based metric which judges the utility of the extracted ego-lane area for driver assistance applications by fitting a driving corridor to the road detection results in the BEV. We believe this to be important for a meaningful evaluation as pixel-level performance is of limited value for vehicle control. State-of-the-art road detection algorithms are used to demonstrate results using classical pixel-level metrics in perspective and BEV space as well as the novel behavior-based performance measure. All data and annotations are made publicly available on the KITTI online evaluation website in order to serve as a common benchmark for road terrain detection algorithms.},
	keywords={},
	doi={10.1109/ITSC.2013.6728473},
	ISSN={2153-0017},
	month={10},
}

https://www.cvlibs.net/datasets/kitti/

@misc{dataset_KITTI_web,
	title={KITTI datasety},
	url={https://www.cvlibs.net/datasets/kitti/},
	urldate={2023-07-26}
}

@misc{dataset_woodscape,
	title={Valeo Woodscape dataset},
	url={https://woodscape.valeo.com/woodscape/},
	urldate={2023-07-26}
}

@misc{dataset_A2D2,
	title={Audi A2D2 dataset},
	url={https://www.a2d2.audi/a2d2/en.html},
	urldate={2023-07-26}
}

@ARTICLE{dataset_ApolloScape_paper,
	author={Huang, Xinyu and Wang, Peng and Cheng, Xinjing and Zhou, Dingfu and Geng, Qichuan and Yang, Ruigang},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	title={The ApolloScape Open Dataset for Autonomous Driving and Its Application}, 
	year={2020},
	volume={42},
	number={10},
	pages={2702-2719},
	abstract={Autonomous driving has attracted tremendous attention especially in the past few years. The key techniques for a self-driving car include solving tasks like 3D map construction, self-localization, parsing the driving road and understanding objects, which enable vehicles to reason and act. However, large scale data set for training and system evaluation is still a bottleneck for developing robust perception models. In this paper, we present the ApolloScape dataset [1] and its applications for autonomous driving. Compared with existing public datasets from real scenes, e.g., KITTI [2] or Cityscapes [3] , ApolloScape contains much large and richer labelling including holistic semantic dense point cloud for each site, stereo, per-pixel semantic labelling, lanemark labelling, instance segmentation, 3D car instance, high accurate location for every frame in various driving videos from multiple sites, cities and daytimes. For each task, it contains at lease 15x larger amount of images than SOTA datasets. To label such a complete dataset, we develop various tools and algorithms specified for each task to accelerate the labelling process, such as joint 3D-2D segment labeling, active labelling in videos etc. Depend on ApolloScape, we are able to develop algorithms jointly consider the learning and inference of multiple tasks. In this paper, we provide a sensor fusion scheme integrating camera videos, consumer-grade motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robust self-localization and semantic segmentation for autonomous driving. We show that practically, sensor fusion and joint learning of multiple tasks are beneficial to achieve a more robust and accurate system. We expect our dataset and proposed relevant algorithms can support and motivate researchers for further development of multi-sensor fusion and multi-task learning in the field of computer vision.},
	keywords={},
	doi={10.1109/TPAMI.2019.2926463},
	ISSN={1939-3539},
month={Oct},
}

@misc{dataset_ApolloScape,
	title={ApolloScape dataset},
	url={https://apolloscape.auto/index.html},
	urldate={2023-07-26}
}

@INPROCEEDINGS{dataset_Argoverse1_paper,
	author={Chang, Ming-Fang and Lambert, John and Sangkloy, Patsorn and Singh, Jagjeet and Bak, Slawomir and Hartnett, Andrew and Wang, De and Carr, Peter and Lucey, Simon and Ramanan, Deva and Hays, James},
	booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={Argoverse: 3D Tracking and Forecasting With Rich Maps}, 
	year={2019},
	volume={},
	number={},
	pages={8740-8749},
	abstract={We present Argoverse, a dataset designed to support autonomous vehicle perception tasks including 3D tracking and motion forecasting. Argoverse includes sensor data collected by a fleet of autonomous vehicles in Pittsburgh and Miami as well as 3D tracking annotations, 300k extracted interesting vehicle trajectories, and rich semantic maps. The sensor data consists of 360 degree images from 7 cameras with overlapping fields of view, forward-facing stereo imagery, 3D point clouds from long range LiDAR, and 6-DOF pose. Our 290km of mapped lanes contain rich geometric and semantic metadata which are not currently available in any public dataset. All data is released under a Creative Commons license at Argoverse.org. In baseline experiments, we use map information such as lane direction, driveable area, and ground height to improve the accuracy of 3D object tracking. We use 3D object tracking to mine for more than 300k interesting vehicle trajectories to create a trajectory forecasting benchmark. Motion forecasting experiments ranging in complexity from classical methods (k-NN) to LSTMs demonstrate that using detailed vector maps with lane-level information substantially reduces prediction error. Our tracking and forecasting experiments represent only a superficial exploration of the potential of rich maps in robotic perception. We hope that Argoverse will enable the research community to explore these problems in greater depth.},
	keywords={},
	doi={10.1109/CVPR.2019.00895},
	ISSN={2575-7075},
	month={6},
}

@inproceedings{dataset_Argoverse2_paper,
 author = {Wilson, Benjamin and Qi, William and Agarwal, Tanmay and Lambert, John and Singh, Jagjeet and Khandelwal, Siddhesh and Pan, Bowen and Kumar, Ratnesh and Hartnett, Andrew and Kaesemodel Pontes, Jhony and Ramanan, Deva and Carr, Peter and Hays, James},
 booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
 editor = {J. Vanschoren and S. Yeung},
 pages = {},
 publisher = {Curran},
 title = {Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting},
 url = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/4734ba6f3de83d861c3176a6273cac6d-Paper-round2.pdf},
 volume = {1},
 year = {2021}
}
@misc{dataset_Argoverse,
	title={Argoverse dataset},
	url={https://www.argoverse.org/index.html},
	urldate={2023-07-26}
}


%@misc{,
%	title={},
%	url={},
%	urldate={2023-07-26}
%}
